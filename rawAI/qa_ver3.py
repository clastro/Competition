# -*- coding: utf-8 -*-
"""QA_ver3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CB07lsvxd_2LoZ9tDWF4bgjgohUxpT63
"""

import pandas as pd
import numpy as np
import re

!pip install transformers
from transformers import pipeline
# string difference
from difflib import SequenceMatcher

### Load Question Answering Model Pre-trained

#qa_model = pipeline("question-answering", model='deepset/deberta-v3-base-squad2', device=0)
qa_model = pipeline("question-answering", model='distilbert-base-uncased-distilled-squad', device=0)
#qa_model = pipeline("question-answering", model='bert-large-uncased-whole-word-masking-finetuned-squad')

#from transformers import DistilBertForQuestionAnswering
#model = DistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

#from transformers import TFDistilBertForQuestionAnswering
#qa_model2 = TFDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

"""일단 데이터 형태를 봄"""

df_train = pd.read_csv('train.csv')

"""Preprocessing"""

def clean_text(inputString):
  inputString = inputString.replace('et al.','')
  inputString = inputString.replace('No.','')
  inputString = inputString.replace('Inc.','')
  inputString = re.sub('[0-9]', '', inputString)
  return re.sub('[-=+#/\?:^@*\",※~ㆍ!』\(\)‘|`…》\t\n\”\“·]', '', inputString)

df_train['first_party'] = df_train['first_party'].apply(lambda x : clean_text(x))
df_train['second_party'] = df_train['second_party'].apply(lambda x : clean_text(x))
df_train['facts'] = df_train['facts'].apply(lambda x : clean_text(x))

df_train

"""QA Modeling


"""

def analyze_model(first_party,second_party,content):
    question = "Who won the trial, " + first_party + " or "+ second_party +"?"
    return qa_model(question = question, context = content)['answer']

def make_question(first_party,second_party):
    question = "Who won the trial, " + first_party + " or "+ second_party +"?"
    return question

df_train['answer'] = df_train.apply(lambda x : analyze_model(x.first_party, x.second_party, x.facts), axis = 1)

df_train['questions'] = df_train.apply(lambda x : make_question(x.first_party, x.second_party), axis = 1)

# Keras will assign a separate loss for each output and add them together. So we'll just use the standard CE loss
# instead of using the built-in model.compute_loss, which expects a dict of outputs and averages the two terms.
# Note that this means the loss will be 2x of when using TFTrainer since we're adding instead of averaging them.
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
#qa_model.distilbert.return_dict = True # if using 🤗 Transformers >3.02, make sure outputs are tuples

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
qa_model.compile(optimizer=optimizer, loss=loss) # can also use any keras loss fn
qa_model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)

QuestionAnsweringPipeline.

from transformers import TFTrainer, TFTrainingArguments

training_args = TFTrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
)

#model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

trainer = TFTrainer(
    model=qa_model,                         # the instantiated 🤗 Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=valid_dataset             # evaluation dataset
)

trainer.train()

def str_similarity(first,second,answer):
    first_result = SequenceMatcher(None, first, answer).ratio()
    second_result = SequenceMatcher(None, second, answer).ratio()

    if(first_result > second_result):
        res = np.ceil(first_result)
    else:
        res = 0
    return res

df_train['first_sim'] = df_train.apply(lambda x: str_similarity(x.first_party, x.second_party, x.answer), axis=1)

df_train

np.mean(df_train['first_party_winner'] == df_train['first_sim'])

df_train

df_test = pd.read_csv('test.csv')

df_test

df_test['first_party'] = df_test['first_party'].apply(lambda x : clean_text(x))
df_test['second_party'] = df_test['second_party'].apply(lambda x : clean_text(x))
df_test['facts'] = df_test['facts'].apply(lambda x : clean_text(x))

df_test

#df_test['answer'] = np.vectorize(analyze_model)(df_test['first_party'],df_test['second_party'],df_test['facts'])

df_test['answer'] = df_test.apply(lambda x : analyze_model(x.first_party, x.second_party, x.facts), axis = 1)

df_test['first_sim'] = df_test.apply(lambda x: str_similarity(x.first_party, x.second_party, x.answer), axis=1)

df_test

df_test

df_test #Yes or No

"""ToDoList
1. Train Data를 FineTuning
2. First party or Second party를 본문에 나오는 형태로 변환해야 함
3. 전처리 : 개행 문자 제거 \t, \n : Completed

Discussion
1. QA 모델의 질문을 어떻게 바꾸면 성능이 좋아질까?
2. First party를 기준으로 능동태로 문장을 변환하고 Binary Classification으로 문제를 푼다면 좋을텐데 방법이?
3. 아직도 QA 모델은 사람 이름을 정확히 인식하지 못 하는데 어떻게 전처리 해야 할까?
"""

df_test['facts'][4] ### 이름에 Berger만 나옴

clean_text(df_test['facts'][1236])

clean_text(df_test['facts'][1237])

submit = pd.read_csv('./sample_submission.csv')

submit['first_party_winner'] = df_test['first_sim']

submit.to_csv('./light_bert_confirm_prepro.csv', index=False)

