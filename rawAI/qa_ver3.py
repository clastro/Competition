# -*- coding: utf-8 -*-
"""QA_ver3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CB07lsvxd_2LoZ9tDWF4bgjgohUxpT63
"""

import pandas as pd
import numpy as np
import re

!pip install transformers
from transformers import pipeline
# string difference
from difflib import SequenceMatcher

### Load Question Answering Model Pre-trained

#qa_model = pipeline("question-answering", model='deepset/deberta-v3-base-squad2', device=0)
qa_model = pipeline("question-answering", model='distilbert-base-uncased-distilled-squad', device=0)
#qa_model = pipeline("question-answering", model='bert-large-uncased-whole-word-masking-finetuned-squad')

#from transformers import DistilBertForQuestionAnswering
#model = DistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

#from transformers import TFDistilBertForQuestionAnswering
#qa_model2 = TFDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

"""ì¼ë‹¨ ë°ì´í„° í˜•íƒœë¥¼ ë´„"""

df_train = pd.read_csv('train.csv')

"""Preprocessing"""

def clean_text(inputString):
  inputString = inputString.replace('et al.','')
  inputString = inputString.replace('No.','')
  inputString = inputString.replace('Inc.','')
  inputString = re.sub('[0-9]', '', inputString)
  return re.sub('[-=+#/\?:^@*\",â€»~ã†!ã€\(\)â€˜|`â€¦ã€‹\t\n\â€\â€œÂ·]', '', inputString)

df_train['first_party'] = df_train['first_party'].apply(lambda x : clean_text(x))
df_train['second_party'] = df_train['second_party'].apply(lambda x : clean_text(x))
df_train['facts'] = df_train['facts'].apply(lambda x : clean_text(x))

df_train

"""QA Modeling


"""

def analyze_model(first_party,second_party,content):
    question = "Who won the trial, " + first_party + " or "+ second_party +"?"
    return qa_model(question = question, context = content)['answer']

def make_question(first_party,second_party):
    question = "Who won the trial, " + first_party + " or "+ second_party +"?"
    return question

df_train['answer'] = df_train.apply(lambda x : analyze_model(x.first_party, x.second_party, x.facts), axis = 1)

df_train['questions'] = df_train.apply(lambda x : make_question(x.first_party, x.second_party), axis = 1)

# Keras will assign a separate loss for each output and add them together. So we'll just use the standard CE loss
# instead of using the built-in model.compute_loss, which expects a dict of outputs and averages the two terms.
# Note that this means the loss will be 2x of when using TFTrainer since we're adding instead of averaging them.
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
#qa_model.distilbert.return_dict = True # if using ğŸ¤— Transformers >3.02, make sure outputs are tuples

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
qa_model.compile(optimizer=optimizer, loss=loss) # can also use any keras loss fn
qa_model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)

QuestionAnsweringPipeline.

from transformers import TFTrainer, TFTrainingArguments

training_args = TFTrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
)

#model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

trainer = TFTrainer(
    model=qa_model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=valid_dataset             # evaluation dataset
)

trainer.train()

def str_similarity(first,second,answer):
    first_result = SequenceMatcher(None, first, answer).ratio()
    second_result = SequenceMatcher(None, second, answer).ratio()

    if(first_result > second_result):
        res = np.ceil(first_result)
    else:
        res = 0
    return res

df_train['first_sim'] = df_train.apply(lambda x: str_similarity(x.first_party, x.second_party, x.answer), axis=1)

df_train

np.mean(df_train['first_party_winner'] == df_train['first_sim'])

df_train

df_test = pd.read_csv('test.csv')

df_test

df_test['first_party'] = df_test['first_party'].apply(lambda x : clean_text(x))
df_test['second_party'] = df_test['second_party'].apply(lambda x : clean_text(x))
df_test['facts'] = df_test['facts'].apply(lambda x : clean_text(x))

df_test

#df_test['answer'] = np.vectorize(analyze_model)(df_test['first_party'],df_test['second_party'],df_test['facts'])

df_test['answer'] = df_test.apply(lambda x : analyze_model(x.first_party, x.second_party, x.facts), axis = 1)

df_test['first_sim'] = df_test.apply(lambda x: str_similarity(x.first_party, x.second_party, x.answer), axis=1)

df_test

df_test

df_test #Yes or No

"""ToDoList
1. Train Dataë¥¼ FineTuning
2. First party or Second partyë¥¼ ë³¸ë¬¸ì— ë‚˜ì˜¤ëŠ” í˜•íƒœë¡œ ë³€í™˜í•´ì•¼ í•¨
3. ì „ì²˜ë¦¬ : ê°œí–‰ ë¬¸ì ì œê±° \t, \n : Completed

Discussion
1. QA ëª¨ë¸ì˜ ì§ˆë¬¸ì„ ì–´ë–»ê²Œ ë°”ê¾¸ë©´ ì„±ëŠ¥ì´ ì¢‹ì•„ì§ˆê¹Œ?
2. First partyë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëŠ¥ë™íƒœë¡œ ë¬¸ì¥ì„ ë³€í™˜í•˜ê³  Binary Classificationìœ¼ë¡œ ë¬¸ì œë¥¼ í‘¼ë‹¤ë©´ ì¢‹ì„í…ë° ë°©ë²•ì´?
3. ì•„ì§ë„ QA ëª¨ë¸ì€ ì‚¬ëŒ ì´ë¦„ì„ ì •í™•íˆ ì¸ì‹í•˜ì§€ ëª» í•˜ëŠ”ë° ì–´ë–»ê²Œ ì „ì²˜ë¦¬ í•´ì•¼ í• ê¹Œ?
"""

df_test['facts'][4] ### ì´ë¦„ì— Bergerë§Œ ë‚˜ì˜´

clean_text(df_test['facts'][1236])

clean_text(df_test['facts'][1237])

submit = pd.read_csv('./sample_submission.csv')

submit['first_party_winner'] = df_test['first_sim']

submit.to_csv('./light_bert_confirm_prepro.csv', index=False)

